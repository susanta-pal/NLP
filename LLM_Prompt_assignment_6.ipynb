{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTR8RX0IFchXsoRi2CaI/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susanta-pal/NLP/blob/main/LLM_Prompt_assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xyfSzkYO2px",
        "outputId": "b27cec3a-f871-486d-c9b2-a0ff67a35447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the president of USA?\n",
            "The president is Narendra Modi.\n",
            "NO\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This program is build with Flan-T5-XL LLM to be able to determine whether an answer to a query is correct or not.\n",
        "\n",
        "It accepts two parameters provided as a command line input.\n",
        "The first input represents the query and the second input is the answer.\n",
        "If the answer is correct for the query then the output should be 'YES' else 'NO'.\n",
        "\n",
        "Syntax: python template.py <string> <string>\n",
        "\n",
        "The following example is given for your reference:\n",
        "\n",
        "Terminal: python template.py 'Who is Rabindranath Tagore?' 'He is a very famous football player.'\n",
        "  Output: NO\n",
        "\n",
        "Terminal: python template.py 'Who is Rabindranath Tagore?' 'He is a famous poet, writer, playwright, composer, philosopher, social reformer, and painter of the Bengal Renaissance.'\n",
        "  Output: YES\n",
        "\n",
        "You are expected to create some examples of your own to test the correctness of your pipeline.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "ALERT: * * * No changes are allowed to import statements  * * *\n",
        "\"\"\"\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import re\n",
        "\n",
        "##### You may comment this section to see verbose -- but you must un-comment this before final submission. ######\n",
        "transformers.logging.set_verbosity_error()\n",
        "transformers.utils.logging.disable_progress_bar()\n",
        "#################################################################################################################\n",
        "\n",
        "\"\"\"\n",
        "* * * Changes allowed from here  * * *\n",
        "\"\"\"\n",
        "def clean_decoded_output(decoded_output):\n",
        "    # clean the output from LLM and return\n",
        "\n",
        "    # Strip, convert to upper-case, and remove any non-alphabetic characters except 'YES' or 'NO'\n",
        "    decoded_output = decoded_output.strip().upper()\n",
        "    if \"YES\" in decoded_output:\n",
        "        return \"YES\"\n",
        "    elif \"NO\" in decoded_output:\n",
        "        return \"NO\"\n",
        "    else:\n",
        "        return \"NO\"  # Default to NO if unclear\n",
        "\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def llm_function(model,tokenizer,a,b):\n",
        "\n",
        "     # Formulate the prompt\n",
        "    input_text = f\"Question: {a} Answer: {b} Is this answer correct? Respond with YES or NO.\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Generate the output from the model\n",
        "    outputs = model.generate(input_ids)\n",
        "\n",
        "    # Decode the output\n",
        "    decoded_output = tokenizer.decode(outputs[0])\n",
        "\n",
        "    # Clean and return the output\n",
        "    cleaned_output = clean_decoded_output(decoded_output)\n",
        "\n",
        "\n",
        "    '''\n",
        "    The steps are given for your reference:\n",
        "\n",
        "    1. Properly formulate the prompt as per the question - which should output either 'YES' or 'NO'. The output must always be upper-case. You may post-process to get the desired output.\n",
        "    2. Tokenize the prompt\n",
        "    3. Pass the tokenized prompt to the model and generate output. You can use default hyper-parameters for the model.\n",
        "    4. Decode the model output.\n",
        "    5. Clean output and return.\n",
        "\n",
        "    Note: The model (Flan-T5-XL) and tokenizer is already initialized. Do not modify that section.\n",
        "    '''\n",
        "\n",
        "    return cleaned_output\n",
        "\n",
        "\"\"\"\n",
        "ALERT: * * * No changes are allowed below this comment  * * *\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #input_data_one = sys.argv[1].strip()\n",
        "    #input_data_two = sys.argv[2].strip()\n",
        "    input_data_one = input()\n",
        "    input_data_two = input()\n",
        "\n",
        "\n",
        "    ##################### Loading Model and Tokenizer ########################\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
        "    ##########################################################################\n",
        "\n",
        "    \"\"\"  Call to function that will perform the computation. \"\"\"\n",
        "\n",
        "    a = input_data_one\n",
        "    b = input_data_two\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    out = llm_function(model,tokenizer,a,b)\n",
        "    print(out.strip())\n",
        "\n",
        "    \"\"\" End to call \"\"\""
      ]
    }
  ]
}